{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerical Differentiation\n",
    "========================\n",
    "Numeric differentiation is a technique used to approximate the derivative of a function when an analytical derivative is difficult or impossible to obtain. It is widely used to analyze rates of change in discrete data sets or complex functions. The most common methods include finite difference approximations, such as **forward**, **backward**, and **central** differences, which estimate derivatives using function values at nearby points. While numerical differentiation is straightforward to implement, it can introduce errors due to finite precision and step size selection, requiring careful consideration of accuracy and stability.\n",
    "\n",
    "## Forward/Backward Difference \n",
    "\n",
    "The forward/backward difference uses the traditional equation for differentiation:\n",
    "\n",
    "$$\\frac{dy}{dx} = y'(x) =  \\frac{y(x+\\Delta x) - y(x)}{h}$$\n",
    "\n",
    "where $h$ is the **step size**, also denoted as $dx\\approx\\Delta x$ in some text.\n",
    "\n",
    "In order to numerically evaluate a derivative $y'(x)=dy/dx$ at point $x_0$, we approximate is by using finite differences.\n",
    "Therefore we find: \n",
    "\n",
    "$dx \\approx \\Delta x =x_1-x_0 = h$\n",
    "\n",
    "$dy \\approx \\Delta y =y_1-y_0= y(x_1)-y(x_0) = y(x_0+\\Delta x)-y(x_0)$\n",
    "\n",
    "Then we re-write the derivative in terms of discrete differences as:\n",
    "$$\\frac{dy}{dx} \\approx \\frac{\\Delta y}{h}$$\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let's look at the accuracy of this approximation in terms of the interval $\\Delta x$. In our first example we will evaluate the derivative of $y=x^2$ at $x=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dx = 1.\n",
    "x = 1.\n",
    "ans = 2.\n",
    "step = []\n",
    "er = []\n",
    "while(dx > 1.e-16):\n",
    "    dy = (x+dx)*(x+dx)-x*x\n",
    "    d = dy / dx\n",
    "    step.append(dx)\n",
    "    er.append(d-ans)\n",
    "    print(\"%6.0e %20.16f %20.16f\" % (dx, d, d-ans))\n",
    "    dx = dx / 10.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# need absolute values for log plot\n",
    "ers = [abs(x) for x in er ]\n",
    "plt.loglog(step,ers)\n",
    "plt.xlabel('step size')\n",
    "plt.ylabel('error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is it that the sequence does not converge? This is due to the round-off errors in the representation of the floating point numbers. To see this, we can simply type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((1.+0.0001)*(1+0.0001)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using powers of 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 1.\n",
    "x = 1.\n",
    "step2 = []\n",
    "er2 = []\n",
    "while(dx > 1.e-16):\n",
    "    dy = (x+dx)*(x+dx)-x*x\n",
    "    d = dy / dx\n",
    "    step2.append(dx)\n",
    "    er2.append(d-2)\n",
    "    print(\"%8.5e %20.16f %20.16f\" % (dx, d, d-2.))\n",
    "    dx = dx / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# need absolute values for log plot\n",
    "ers2 = [abs(x) for x in er2 ]\n",
    "plt.loglog(step2,ers2)\n",
    "plt.xlabel('step size')\n",
    "plt.ylabel('error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That appeared to have less trouble as the step size got smaller. Why is that? To answer, consider the output from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{0.1:.60f}\")  # Shows it's not exactly 0.1\n",
    "print(f\"{0.5:.60f}\")  # Shows it IS exactly 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central (Midpoint) Difference\n",
    "\n",
    "The central difference method is a more accurate numerical differentiation technique compared to forward or backward differences because it utilizes points on both sides of the target point to estimate the derivative. It is given by the formula:\n",
    "$$ \\frac{dy}{dx} \\approx \\frac{y(x_0+\\frac{h}{2})-y(x_0-\\frac{h}{2})}{h}.$$\n",
    "By averaging symmetric function values around $x$, the central difference method reduces truncation error to the order of $\\mathcal{O}(h^2)$\n",
    "\n",
    "\n",
    "For a more complex function we may need to import it from the math module. For instance, let's calculate the derivative of $sin(x)$ at $x=\\pi/4$, including both the forward and central differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sin, sqrt, pi\n",
    "dx = 1.\n",
    "data = []\n",
    "while(dx > 1.e-16):\n",
    "    x = pi/4.\n",
    "    d1 = sin(x+dx) - sin(x); #forward\n",
    "    d2 = sin(x+dx*0.5) - sin(x-dx*0.5); # midpoint\n",
    "    d1 = d1 / dx;\n",
    "    d2 = d2 / dx;\n",
    "    e1 = d1-sqrt(2.)/2.\n",
    "    e2 = d2-sqrt(2.)/2.\n",
    "    print(\"%8.5e %20.16f %20.16f %20.16f %20.16f\" % (dx, d1, e1, d2, e2) )\n",
    "    data.append([dx,d1,e1,d2,e2])\n",
    "    dx = dx / 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arraydata = np.array(data)\n",
    "arraydata[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.loglog(arraydata[:,0],abs(arraydata[:,2]),'r--',label= 'Forward Diff' )\n",
    "plt.loglog(arraydata[:,0],abs(arraydata[:,4]),'b', label='Central Diff')\n",
    "plt.xlabel('step size')\n",
    "plt.ylabel('error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you notice? Which one does \"better\"? What does *better* mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Error Behavior\n",
    "\n",
    "Notice something strange in the plot above? The error **decreases** as we make the step size smaller, but then it suddenly starts **increasing** again when the step size gets very small! Why?\n",
    "\n",
    "There are actually **two sources of error** competing with each other:\n",
    "\n",
    "### 1. Truncation Error (Mathematical Approximation)\n",
    "This comes from the fact that our finite difference formula is an *approximation* to the true derivative. The Taylor series expansion shows:\n",
    "$$f(x+h) = f(x) + f'(x)h + \\frac{1}{2}f''(x)h^2 + \\mathcal{O}(h^3)$$\n",
    "\n",
    "For forward difference:\n",
    "$$\\frac{f(x+h)-f(x)}{h} = f'(x) + \\frac{1}{2}f''(x)h + \\mathcal{O}(h^2)$$\n",
    "\n",
    "The truncation error is proportional to **h** (for forward/backward) or **h²** (for central difference).\n",
    "- Behavior: Gets *smaller* as h decreases\n",
    "- This is why we expect better accuracy with smaller step sizes\n",
    "\n",
    "### 2. Roundoff Error (Computer Arithmetic)\n",
    "Computers can only store numbers to finite precision (about 16 decimal digits for standard floats). When we compute $f(x+h) - f(x)$ for very small h, both numbers are nearly identical, and we lose significant digits in the subtraction. This is called **catastrophic cancellation**.\n",
    "\n",
    "- Behavior: Gets *larger* as $h$ decreases (we're subtracting nearly equal numbers)\n",
    "- This is why the error explodes when $h$ becomes too small\n",
    "\n",
    "### The Optimal Step Size\n",
    "The best accuracy occurs where these two errors balance each other. For double precision arithmetic:\n",
    "- Forward/backward difference: optimal $h\\approx \\sqrt{\\varepsilon}\\approx 10^{-8}$\n",
    "- Central difference: optimal $h\\approx \\sqrt[3]{\\varepsilon}\\approx 10^{-5}$\n",
    "\n",
    "Let's visualize both error sources together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate derivative of x^2 at x=1 (true answer = 2)\n",
    "x = 1.0\n",
    "step_sizes = np.logspace(0, -16, 100)  # h from 1 to 10^-16\n",
    "forward_errors = []\n",
    "truncation_est = []\n",
    "roundoff_est = []\n",
    "\n",
    "for h in step_sizes:\n",
    "    # Forward difference\n",
    "    dy = (x+h)*(x+h) - x*x\n",
    "    derivative_approx = dy / h\n",
    "    error = abs(derivative_approx - 2.0)\n",
    "    forward_errors.append(error)\n",
    "    \n",
    "    # Estimate truncation error: proportional to h\n",
    "    # For f(x)=x^2, f''(x)=2, so truncation error ≈ h\n",
    "    truncation_est.append(h)\n",
    "    \n",
    "    # Estimate roundoff error: proportional to ε/h\n",
    "    # where ε ≈ 10^-16 is machine epsilon\n",
    "    machine_eps = np.finfo(float).eps\n",
    "    roundoff_est.append(2 * machine_eps / h)  # factor of 2 from the subtraction\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(step_sizes, forward_errors, 'b-', linewidth=2, label='Actual Error')\n",
    "plt.loglog(step_sizes, truncation_est, 'r--', linewidth=2, label='Truncation Error ∝ h')\n",
    "plt.loglog(step_sizes, roundoff_est, 'g--', linewidth=2, label='Roundoff Error ∝ ε/h')\n",
    "\n",
    "# Mark the optimal step size\n",
    "optimal_h = np.sqrt(machine_eps) * 2  # rough estimate\n",
    "plt.axvline(optimal_h, color='orange', linestyle=':', linewidth=2, label=f'Optimal h ≈ {optimal_h:.1e}')\n",
    "\n",
    "plt.xlabel('Step size h', fontsize=12)\n",
    "plt.ylabel('Absolute Error', fontsize=12)\n",
    "plt.title('Truncation vs Roundoff Error in Numerical Differentiation', fontsize=14)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Machine epsilon: {machine_eps:.2e}\")\n",
    "print(f\"Optimal step size estimate: {optimal_h:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special functions in **numpy**\n",
    "\n",
    "NumPy provides built-in functions for numerical differentiation that are convenient when working with array data:\n",
    "\n",
    "- **`np.diff()`**: Calculates forward differences between consecutive elements\n",
    "- **`np.gradient()`**: Calculates derivatives using central differences (more accurate)\n",
    "\n",
    "Let's understand how each works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple example: derivative of x^2\n",
    "x = np.array([0, 1, 2, 3, 4, 5])\n",
    "y = x**2  # y = [0, 1, 4, 9, 16, 25]\n",
    "\n",
    "print(\"x values:\", x)\n",
    "print(\"y values:\", y)\n",
    "print()\n",
    "\n",
    "# np.diff() computes forward differences: y[i+1] - y[i]\n",
    "dy = np.diff(y)\n",
    "print(\"np.diff(y):\", dy)\n",
    "print(\"Note: output has one fewer element than input!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: `np.diff()` returns an array with **one fewer element** than the input because it computes differences *between* consecutive points. To get a derivative, we also need to divide by the step size:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = np.diff(x)\n",
    "derivative_forward = dy / dx\n",
    "print(\"Derivative using np.diff():\", derivative_forward)\n",
    "print(\"True derivative at those points:\", 2*x[:-1])  # d(x^2)/dx = 2x, evaluated at x[0] through x[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at `np.gradient()`, which is more sophisticated and typically more accurate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.gradient() returns the same number of elements as the input\n",
    "derivative_gradient = np.gradient(y, x)\n",
    "\n",
    "print(\"x values:                    \", x)\n",
    "print(\"Derivative using np.gradient:\", derivative_gradient)\n",
    "print(\"True derivative (2x):        \", 2*x)\n",
    "print(\"Notice: gradient gives us a derivative at EVERY point, including the endpoints!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does `np.gradient()` work?\n",
    "\n",
    "`np.gradient()` is smarter about computing derivatives:\n",
    "\n",
    "1. **At interior points** (not at the edges): Uses **central difference**\n",
    "   $$f'(x_i) \\approx \\frac{f(x_{i+1}) - f(x_{i-1})}{x_{i+1} - x_{i-1}}$$\n",
    "   \n",
    "2. **At the first point**: Uses **forward difference**\n",
    "   $$f'(x_0) \\approx \\frac{f(x_1) - f(x_0)}{x_1 - x_0}$$\n",
    "   \n",
    "3. **At the last point**: Uses **backward difference**\n",
    "   $$f'(x_N) \\approx \\frac{f(x_N) - f(x_{N-1})}{x_N - x_{N-1}}$$\n",
    "\n",
    "This boundary handling is automatic and gives us derivatives at all points, making it very convenient for data analysis.\n",
    "\n",
    "Let's verify this behavior manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "\n",
    "y = lambda x: x*x\n",
    "\n",
    "x1 = np.arange(0,10,1)\n",
    "x2 = np.arange(0,10,0.1)\n",
    "\n",
    "y1 = np.gradient(y(x1), 1.)\n",
    "print(y1)\n",
    "print(np.diff(y1))\n",
    "\n",
    "pyplot.plot(x1,np.gradient(y(x1),1.),'r--o');\n",
    "pyplot.plot(x1[:x1.size-1],np.diff(y(x1))/np.diff(x1),'b--x');\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use which function?\n",
    "\n",
    "- Use `np.diff()` when:\n",
    "  1. You specifically need the forward difference\n",
    "  2. You're computing differences between consecutive data points (not necessarily derivatives)\n",
    "  \n",
    "- Use `np.gradient()` when:\n",
    "  1. You want derivatives at all points in your array (including endpoints)\n",
    "  1. You want better accuracy (central differences in the interior)\n",
    "  1. You're analyzing experimental or simulation data\n",
    "  \n",
    "**In most cases, `np.gradient()` is the better choice for numerical differentiation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond 1D: Gradients in Multiple Dimensions\n",
    "\n",
    "So far we've computed derivatives of functions that depend on a single variable: $f(x)$. But in physics, many quantities depend on multiple variables - for example, temperature $T(x,y,z)$ varies throughout space, or potential energy $U(x,y)$ depends on position in a plane.\n",
    "\n",
    "When we have a function of multiple variables, $f(x,y)$, the gradient is a vector that points in the direction of steepest increase:\n",
    "\n",
    "$$\\nabla f = \\left(\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}\\right)$$\n",
    "\n",
    "This is really just two separate derivatives (called partial derivatives) - one with respect to x and one with respect to y.\n",
    "\n",
    "#### Physical Examples of Gradients:\n",
    "\n",
    "1. **Electric Field**: The electric field is the negative gradient of electric potential\n",
    "   $$\\vec{E} = -\\nabla V$$\n",
    "   The field points in the direction where potential decreases most rapidly.\n",
    "\n",
    "2. **Force from Potential Energy**: Force is the negative gradient of potential energy\n",
    "   $$\\vec{F} = -\\nabla U$$\n",
    "   Objects naturally move \"downhill\" in potential energy.\n",
    "\n",
    "3. **Heat Flow**: Heat flows in the direction of the negative temperature gradient\n",
    "   $$\\vec{q} = -k\\nabla T$$\n",
    "   Heat moves from hot to cold regions.\n",
    "\n",
    "4. **Pressure Gradients**: Drive fluid flow and wind patterns in meteorology\n",
    "   $$\\vec{F} = -\\nabla P$$\n",
    "\n",
    "The good news: `np.gradient()` can compute these multidimensional derivatives automatically!\n",
    "\n",
    "#### Example: Gradient of a 2D Gaussian\n",
    "\n",
    "Let's visualize the gradient of a function that looks like a hill: $f(x,y) = e^{-(x^2+y^2)}$\n",
    "\n",
    "This could represent:\n",
    "- Temperature distribution around a hot spot\n",
    "- Concentration of a chemical diffusing from a point source\n",
    "- Probability density of finding a particle near the origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear in x\n",
    "fx = np.array([[1, 2, 3],\n",
    "               [1, 2, 3],\n",
    "               [1, 2, 3]])\n",
    "\n",
    "# Linear in y  \n",
    "fy = np.array([[1, 1, 1],\n",
    "               [2, 2, 2],\n",
    "               [3, 3, 3]])\n",
    "\n",
    "# Combined - diagonal ramp\n",
    "f_both = fx + fy\n",
    "print(f_both)\n",
    "print(\"Just x-component:\", np.gradient(fx))\n",
    "print(\"Just y-component:\", np.gradient(fy))\n",
    "print(\"Both components:\", np.gradient(f_both))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define grid size\n",
    "x = np.linspace(-2, 2, 20)  # 20 points from -2 to 2\n",
    "y = np.linspace(-2, 2, 20)\n",
    "\n",
    "# Create meshgrid\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Define function f(x, y) = exp(-x^2 - y^2)\n",
    "# This is like a \"hill\" or \"hot spot\" centered at the origin\n",
    "F = np.exp(-(X**2 + Y**2))\n",
    "\n",
    "# Compute numerical gradient\n",
    "# Computes gradient with respect to x and y\n",
    "# (Note: first output is df/dy because arrays are indexed [row, column])\n",
    "dFdx, dFdy = np.gradient(F, x, y) \n",
    "\n",
    "# Plot function\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "# Filled contour plot shows the function value (like a topographic map)\n",
    "ax.contourf(X, Y, F, levels=20, cmap=\"plasma\")  \n",
    "# Quiver plot shows gradient vectors (direction of steepest ascent)\n",
    "# The arrows point \"uphill\" toward higher values of f\n",
    "ax.quiver(X, Y, dFdx, dFdy, color='white') \n",
    "ax.set_title(\"Gradient of a Gaussian Function\")\n",
    "ax.set_xlabel(\"x-axis\")\n",
    "ax.set_ylabel(\"y-axis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Key Takeaway:\n",
    "\n",
    "Just like in 1D, `np.gradient()` works in multiple dimensions. You give it a 2D array and the spacing in each direction, and it returns the partial derivatives. This is incredibly useful for analyzing:\n",
    "- Experimental data on a grid (temperature measurements, pressure readings, etc.)\n",
    "- Simulation results (fluid flow, electromagnetic fields, etc.)\n",
    "- Image processing (edge detection uses gradients!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic Differentiation with SymPy\n",
    "\n",
    "All the methods we've discussed so far are numerical differentiation - they approximate derivatives using finite differences and are subject to truncation and roundoff errors.\n",
    "\n",
    "But sometimes we don't need an approximation. If we can write our function as a mathematical formula (like $x^2 + 3x + 5$), Python can compute the *exact* derivative symbolically, just like you would do by hand in calculus class!\n",
    "\n",
    "This is called *symbolic computation*, and Python's `sympy` library handles it beautifully.\n",
    "\n",
    "### When to use symbolic vs numerical differentiation?\n",
    "\n",
    "**Use symbolic differentiation (SymPy) when:**\n",
    "- You have an explicit formula for your function\n",
    "- You want the exact derivative without any approximation error\n",
    "- You need to manipulate the derivative algebraically\n",
    "\n",
    "**Use numerical differentiation when:**\n",
    "- You only have data points (experimental measurements, simulation output)\n",
    "- Your function is too complicated for symbolic manipulation\n",
    "- You're working with large arrays of numbers\n",
    "\n",
    "Let's see SymPy in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define a symbolic variable (not a number!)\n",
    "x = sp.symbols('x')\n",
    "\n",
    "# Define a function symbolically\n",
    "f = x**2 + 3*x + 5\n",
    "\n",
    "print(\"Function f(x) =\", f)\n",
    "\n",
    "# Compute the derivative symbolically\n",
    "df_dx = sp.diff(f, x)\n",
    "\n",
    "print(\"Derivative df/dx =\", df_dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that SymPy gives us the exact formula for the derivative: $2x + 3$. No approximation, no error!\n",
    "\n",
    "But symbolic expressions aren't very useful if we want to evaluate them at specific points. Let's convert the symbolic derivative to a numerical function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert symbolic expression to a regular Python function\n",
    "f_numeric = sp.lambdify(x, f)\n",
    "df_numeric = sp.lambdify(x, df_dx)\n",
    "\n",
    "# Now we can evaluate at specific points\n",
    "x_val = 2.0\n",
    "print(f\"At x = {x_val}:\")\n",
    "print(f\"  f(x) = {f_numeric(x_val)}\")\n",
    "print(f\"  df/dx = {df_numeric(x_val)}\")\n",
    "\n",
    "# Compare to numerical differentiation\n",
    "h = 0.001\n",
    "numerical_deriv = (f_numeric(x_val + h) - f_numeric(x_val)) / h\n",
    "print(f\"\\nCompare to numerical (forward difference with h={h}):\")\n",
    "print(f\"  df/dx ≈ {numerical_deriv}\")\n",
    "print(f\"  Error: {abs(numerical_deriv - df_numeric(x_val)):.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SymPy handles complicated functions easily\n",
    "import sympy as sp\n",
    "\n",
    "x = sp.symbols('x')\n",
    "g = sp.sin(x) * sp.exp(-x**2)\n",
    "\n",
    "print(\"Function g(x) =\", g)\n",
    "print()\n",
    "\n",
    "# First derivative\n",
    "dg_dx = sp.diff(g, x)\n",
    "print(\"First derivative dg/dx =\", dg_dx)\n",
    "print()\n",
    "\n",
    "# Second derivative (just for fun!)\n",
    "d2g_dx2 = sp.diff(g, x, 2)  # the '2' means differentiate twice\n",
    "print(\"Second derivative d²g/dx² =\", d2g_dx2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Exercise: Analyzing Motion of a Falling Object\n",
    "\n",
    "In this exercise, you'll analyze position data from a falling object to extract its velocity and acceleration using numerical differentiation.\n",
    "\n",
    "## Background\n",
    "\n",
    "A physics student drops a ball from rest and uses a motion sensor to record its position every 0.05 seconds. Due to experimental uncertainties (sensor noise, air resistance, etc.), the measurements aren't perfect. Your job is to:\n",
    "\n",
    "1. Load the experimental data\n",
    "2. Calculate the velocity using numerical differentiation\n",
    "3. Calculate the acceleration from the velocity\n",
    "4. Compare different differentiation methods\n",
    "5. Determine if the motion is consistent with free fall (g ≈ 9.8 m/s²)\n",
    "\n",
    "The data file `falling_ball.txt` contains two columns: time (s) and position (m).\n",
    "\n",
    "### Questions\n",
    "\n",
    "1. Load the data from `falling_ball.txt` and plot position vs time. Does the curve look reasonable for a falling object? What shape do you expect?\n",
    "\n",
    "2. Calculate velocity using the `forward()` and `central()` function from `mydiff.py`. Plot velocity vs time. For free fall from rest, what should the velocity be at t = 0? Does your result match?\n",
    "\n",
    "3. Calculate acceleration by differentiating the velocity (again using `forward()` and `central()`). Plot acceleration vs time and include a horizontal line at g = -9.8 m/s². Calculate the mean acceleration. Does your measured value agree with g = 9.8 m/s²?\n",
    "\n",
    "4. Why is the acceleration plot much noisier than the velocity and position plots?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mydiff import forward, backward, central\n",
    "\n",
    "# Load the data\n",
    "data = np.loadtxt('falling_ball.txt')\n",
    "time = data[:, 0]\n",
    "position = data[:, 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
